{
  "cells": [
    {
      "id": "7240f3d5",
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Tutorial: Comparing Models\n\nThis notebook compares candidate model outputs using the same request and metrics."
    },
    {
      "id": "c2e9832d",
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport plotly.express as px\nfrom pydantic import ValidationError\n\nfrom tollama.core.forecast_metrics import compute_forecast_metrics\nfrom tollama.core.schemas import ForecastRequest, ForecastResponse"
    },
    {
      "id": "1530e4cd",
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "timestamps = [\n    '2025-01-01',\n    '2025-01-02',\n    '2025-01-03',\n    '2025-01-04',\n    '2025-01-05',\n    '2025-01-06',\n]\nrequest_payload = {\n    'model': 'mock',\n    'horizon': 4,\n    'series': [\n        {\n            'id': 'sku_A',\n            'freq': 'D',\n            'timestamps': timestamps,\n            'target': [100, 102, 101, 103, 104, 106],\n            'actuals': [107, 108, 110, 109],\n        }\n    ],\n    'parameters': {'metrics': {'names': ['mae', 'rmse', 'mape'], 'mase_seasonality': 1}},\n    'options': {},\n}\nrequest = ForecastRequest.model_validate(request_payload)\nrequest"
    },
    {
      "id": "80c62df5",
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "candidate_outputs = {\n    'chronos2': [106.8, 108.2, 110.4, 109.4],\n    'timesfm-2.5-200m': [106.0, 107.8, 109.6, 110.1],\n    'granite-ttm-r2': [107.2, 108.5, 109.7, 109.0],\n}\nrows = []\nfor model_name, mean_values in candidate_outputs.items():\n    response = ForecastResponse.model_validate({\n        'model': model_name,\n        'forecasts': [\n            {\n                'id': 'sku_A',\n                'freq': 'D',\n                'start_timestamp': '2025-01-07',\n                'mean': mean_values,\n            }\n        ],\n    })\n    metrics, warnings = compute_forecast_metrics(request=request, response=response)\n    aggregate = metrics.aggregate if metrics is not None else {}\n    rows.append({'model': model_name, **aggregate, 'warnings': '; '.join(warnings)})\n\nresults = pd.DataFrame(rows).sort_values('mae')\nresults"
    },
    {
      "id": "704a8d6e",
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "plot_df = results.melt(\n    id_vars='model',\n    value_vars=['mae', 'rmse', 'mape'],\n    var_name='metric',\n    value_name='value',\n)\nfig, ax = plt.subplots(figsize=(9, 4))\nfor metric in ['mae', 'rmse', 'mape']:\n    subset = plot_df[plot_df['metric'] == metric]\n    ax.plot(subset['model'], subset['value'], marker='o', label=metric)\nax.set_title('Metric comparison by model')\nax.set_ylabel('value')\nax.grid(alpha=0.2)\nax.legend()\nplt.tight_layout()"
    },
    {
      "id": "6fd8ed04",
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "px.bar(\n    plot_df,\n    x='model',\n    y='value',\n    color='metric',\n    barmode='group',\n    title='Model comparison metrics',\n)"
    },
    {
      "id": "a98cce47",
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "bad_payload = dict(request_payload)\nbad_series = dict(request_payload['series'][0])\nbad_series.pop('actuals', None)\nbad_payload['series'] = [bad_series]\ntry:\n    ForecastRequest.model_validate(bad_payload)\nexcept ValidationError as exc:\n    print('Validation error example:', exc.errors()[0]['msg'])"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}